{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: CM vs MN BPNet model training and grid searching\n",
    "\n",
    "The purpose of this notebook is to to train a model on cardiac progenitor and motor neuron progenitor ChIP-seq samples that have been ChIPed for ISL1. We will train a series of models based on targeted parameter searching in order to optimize the settings to be the best possible outputs prior to running TF-MoDISco and performing downstream interpretations. We will use counts correlation scores and auPRC profile classification of summits to determine which combination of parameters are the best. \n",
    "\n",
    "# Computational Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "import warnings;warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.python.util import deprecation; deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "#Modules\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import plotnine\n",
    "from glob import glob\n",
    "from plotnine import *\n",
    "from itertools import product, compress\n",
    "from pybedtools import BedTool\n",
    "from keras import backend as K\n",
    "from bpnet.utils import read_json, create_tf_session\n",
    "from bpnet.dataspecs import DataSpec\n",
    "from bpnet.datasets import StrandedProfile\n",
    "from bpnet.extractors import StrandedBigWigExtractor\n",
    "from bpnet.BPNet import BPNetSeqModel\n",
    "from bpnet.metrics import eval_profile\n",
    "\n",
    "#Setup\n",
    "os.chdir('/n/projects/mw2098/publications/2022_maven_ISL1/')\n",
    "create_tf_session('0', .5)\n",
    "%matplotlib inline\n",
    "\n",
    "#Variables\n",
    "dataspec = DataSpec.load(f'dataspec/dataspec.yml')\n",
    "config = read_json(f'models/seq_width1000-lr0.001-lambda100-n_dil_layers9-conv_kernel_size7-tconv_kernel_size7-filters64/config.gin.json')\n",
    "tasks = list(dataspec.task_specs.keys())\n",
    "\n",
    "with open(r'yml/CM_vs_MN_replicates.yml') as file:\n",
    "    rep_path_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "[[os.path.exists(v2) for k2,v2 in v1.items()] for k1,v1 in rep_path_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p figures/1_model_training_and_grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter given \n",
    "\n",
    "In R, run the following commands in order to remove regions that are on the edges of chromosome boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "source('')\n",
    "rtracklayer::import('bed/I_wt_d6cm_consensus.bed') %>%\n",
    "  check_chromosome_boundaries(., 5000, genome = BSgenome.Hsapiens.UCSC.hg19) %>%\n",
    "  rtracklayer::export(., 'bed/I_wt_d6cm_consensus_filtered.bed')\n",
    "rtracklayer::import('bed/I_wt_s3mn_consensus.bed') %>%\n",
    "  check_chromosome_boundaries(., 5000, genome = BSgenome.Hsapiens.UCSC.hg19) %>%\n",
    "  rtracklayer::export(., 'bed/I_wt_s3mn_consensus_filtered.bed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Address single-bias track\n",
    "\n",
    "Because the original BPNet repository requires only 1 bias track to be used, we merged the WCE input control reads from CM and MN samples to use them as the single bias track. Below is the command used to merge these samples. \n",
    "\n",
    "We investigated both the CM and MN input control samples in order to determine that they did not exibit any cell-specific bias that might indicate that these control samples could not be merged. It was determined that they provide background input information, but that this information was not cell type-specific, especially at sites of high and differential binding in the ISL1 ChIP-seq experimental samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!Rscript scripts/merge_bigwigs.r --files=data/bw/combined/input_wt_*.bw --output=data/bw/combined/input_wt_merged.bw --bsgenome=BSgenome.Hsapiens.UCSC.hg19 --cores 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the evaluation metrics computed in the training step of BPNet, we need to parse through a `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_eval_metrics(eval_path):\n",
    "    from bpnet.utils import read_json\n",
    "    eval_dict = read_json(eval_path)\n",
    "    df = pd.DataFrame([(k,k1,v1) for k,v in eval_dict.items() for k1,v1 in v.items()], columns = ['dataset','id','value'])\n",
    "    df = df.replace({'counts/':'counts//'}, regex=True) \n",
    "    df[['task','head','binsize','metric']] = df.id.str.split('/', expand=True) \n",
    "    df[(df['metric']=='auprc') | (df['metric']=='spearmanr')]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the auPRC measurements from `bpnet.metrics` in order to accommodate ChIP-seq single-channel tracks. Keep in mind threshold values should be fundamentally different from ChIP-nexus data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_seq_counts_amb(x, binsize=2):\n",
    "    \"\"\"Bin the counts\n",
    "    \"\"\"\n",
    "    if binsize == 1:\n",
    "        return x\n",
    "    outlen = x.shape[1] // binsize\n",
    "    xout = np.zeros((x.shape[0], outlen)).astype(float)\n",
    "    for i in range(outlen):\n",
    "        iterval = x[:, (binsize * i):(binsize * (i + 1))]\n",
    "        has_amb = np.any(iterval == -1, axis=1)\n",
    "        has_peak = np.any(iterval == 1, axis=1)\n",
    "        # if no peak and has_amb -> -1\n",
    "        # if no peak and no has_amb -> 0\n",
    "        # if peak -> 1\n",
    "        xout[:, i] = (has_peak - (1 - has_peak) * has_amb).astype(float).flatten()\n",
    "    return xout\n",
    "\n",
    "\n",
    "def eval_seq_profile(yt, yp,\n",
    "                 pos_min_threshold=0.05,\n",
    "                 neg_max_threshold=0.01,\n",
    "                 required_min_pos_counts=2.5,\n",
    "                 binsizes=[1, 2, 4, 10]):\n",
    "    import sklearn.metrics as skm\n",
    "    import logging\n",
    "    import matplotlib.pyplot as plt\n",
    "    from bpnet.utils import read_pkl\n",
    "    from keras.models import load_model\n",
    "    from bpnet.utils import _listify, create_tf_session\n",
    "    from bpnet.stats import permute_array\n",
    "    from bpnet.functions import softmax, mean\n",
    "    import os\n",
    "    import json\n",
    "    from tqdm import tqdm\n",
    "    import matplotlib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from collections import OrderedDict\n",
    "    from bpnet.metrics import bin_counts_amb,bin_counts_max,auprc\n",
    "    import gin\n",
    "    \"\"\"\n",
    "    Evaluate the profile in terms of auPR\n",
    "    Args:\n",
    "      yt: true profile (counts)\n",
    "      yp: predicted profile (fractions)\n",
    "      pos_min_threshold: fraction threshold above which the position is\n",
    "         considered to be a positive\n",
    "      neg_max_threshold: fraction threshold bellow which the position is\n",
    "         considered to be a negative\n",
    "      required_min_pos_counts: smallest number of reads the peak should be\n",
    "         supported by. All regions where 0.05 of the total reads would be\n",
    "         less than required_min_pos_counts are excluded\n",
    "    \"\"\"\n",
    "    # The filtering\n",
    "    # criterion assures that each position in the positive class is\n",
    "    # supported by at least required_min_pos_counts  of reads\n",
    "    do_eval = (yt.mean(axis=1) > required_min_pos_counts / pos_min_threshold).flatten()\n",
    "    # make sure everything sums to one\n",
    "    yp = (yp / yp.sum(axis=1, keepdims=True))\n",
    "    fracs = (yt / yt.sum(axis=1, keepdims=True))\n",
    "    yp_random = permute_array(permute_array(yp[do_eval], axis=1), axis=0)\n",
    "    \n",
    "    out = []\n",
    "    for binsize in binsizes:\n",
    "        is_peak = (fracs >= pos_min_threshold).astype(float)\n",
    "        ambigous = (fracs < pos_min_threshold) & (fracs >= neg_max_threshold)\n",
    "        is_peak[ambigous] = -1\n",
    "        #y_true = np.ravel(bin_seq_counts_amb(is_peak[do_eval], binsize))\n",
    "        y_true = bin_seq_counts_amb(is_peak[do_eval], binsize)\n",
    "        imbalance = np.sum(y_true == 1) / np.sum(y_true >= 0)\n",
    "        n_positives = np.sum(y_true == 1)\n",
    "        n_ambigous = np.sum(y_true == -1)\n",
    "        frac_ambigous = n_ambigous / y_true.size\n",
    "\n",
    "        # TODO - I used to have bin_counts_max over here instead of bin_counts_sum\n",
    "        try:\n",
    "            res = auprc(y_true,\n",
    "                        bin_counts_max(yp[do_eval], binsize))\n",
    "            res_random = auprc(y_true,\n",
    "                               bin_counts_max(yp_random, binsize))\n",
    "        except Exception:\n",
    "            res = np.nan\n",
    "            res_random = np.nan\n",
    "\n",
    "        out.append({\"binsize\": binsize,\n",
    "                    \"auprc\": res,\n",
    "                    \"random_auprc\": res_random,\n",
    "                    \"n_positives\": n_positives,\n",
    "                    \"frac_ambigous\": frac_ambigous,\n",
    "                    \"imbalance\": imbalance\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame.from_dict(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter assessment\n",
    "\n",
    "Here, we aim to look at several parameters:\n",
    "\n",
    "1. `number of filters`\n",
    "2. `learning rate`\n",
    "3. `lambda value weighting the count loss to the profile loss`\n",
    "4. `number of dilational layers`\n",
    "\n",
    "\n",
    "1-4 we will assess in an \"independent fashion\" e.g. how well each deviates from the default settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing parameters independently\n",
    "\n",
    "Number of filters, learning rate, and lambda can be assessed in an independent fashion relative to the recommended defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!python scripts/bpnet_train_as_grid_search.py --dataspec dataspec/dataspec.yml \\\n",
    "--config config/default.gin --output-directory models/ -x .4 --manually-run \\\n",
    "--filters '64' --conv-kernel-size '7' --tconv-kernel-size '7' --n-dil-layers '7' --loss-weight '100' \\\n",
    "--learning-rate '0.01,0.004,0.001,0.0004' --seq-width '1000'\n",
    "\n",
    "!python scripts/bpnet_train_as_grid_search.py --dataspec dataspec/dataspec.yml \\\n",
    "--config config/default.gin --output-directory models/ -x .4 --manually-run \\\n",
    "--filters '64' --conv-kernel-size '7' --tconv-kernel-size '7' --n-dil-layers '7' --loss-weight '1,10,100,250,500,1000' \\\n",
    "--learning-rate '0.001' --seq-width '1000'\n",
    "\n",
    "!python scripts/bpnet_train_as_grid_search.py --dataspec dataspec/dataspec.yml \\\n",
    "--config config/default.gin --output-directory models/ -x .4 --manually-run \\\n",
    "--filters '16,64,128,256,512' --conv-kernel-size '7' --tconv-kernel-size '7' --n-dil-layers '7' --loss-weight '100' \\\n",
    "--learning-rate '0.001' --seq-width '1000'\n",
    "\n",
    "!python scripts/bpnet_train_as_grid_search.py --dataspec dataspec/dataspec.yml \\\n",
    "--config config/default.gin --output-directory models/ -x .4 --manually-run \\\n",
    "--filters '64' --conv-kernel-size '7' --tconv-kernel-size '7' --n-dil-layers '5,7,9,11' --loss-weight '100' \\\n",
    "--learning-rate '0.001' --seq-width '1000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function that plots auPRC and spearman counts correlation to assess differences in tasks across various parameter selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_independent_assessment(glob_str, parameter, output_plot_prefix, model_dir='models/'):\n",
    "    #Collect evaluation metrics\n",
    "    evals_path = glob(os.path.join(model_dir, glob_str))\n",
    "    evals_dict = {i.split('/')[1]: collect_eval_metrics(i) for i in evals_path}\n",
    "\n",
    "    #Convert to pd.df\n",
    "    evals_df = pd.DataFrame()\n",
    "    for k,v in evals_dict.items():\n",
    "        v['model'] = k\n",
    "        evals_df = evals_df.append(v)\n",
    "\n",
    "    #Integrate model information\n",
    "    model_params = ['seq_width','lr','lambda','n_dil_layers','conv_kernal_size','tconv_kernel_size','filters']\n",
    "    evals_df[model_params] = evals_df.model.str.split('-', expand=True)\n",
    "    model_params = model_params + ['binsize']\n",
    "    evals_df[model_params] = evals_df[model_params].replace('[-+A-Za-z]','', regex = True).replace('_','', regex = True).replace('=','', regex = True)\n",
    "    evals_df[model_params] = evals_df[model_params].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "    #Create categorical value\n",
    "    evals_df[parameter] = pd.Categorical(evals_df[parameter], \n",
    "                                      categories=evals_df.sort_values([parameter])[parameter].unique(), \n",
    "                                      ordered=True)\n",
    "    \n",
    "    #Plot Spearman correlation coefficient\n",
    "    plotnine.options.figure_size = (20,3)\n",
    "\n",
    "    spearmanr_plot = (ggplot(data = evals_df[evals_df['metric']=='spearmanr'], mapping = aes(x = 'dataset', y = 'value', fill=parameter))+\n",
    "        geom_bar(stat = 'identity', position = 'dodge')+\n",
    "        facet_grid('. ~ task')+\n",
    "        scale_y_continuous(name = 'Cor')+\n",
    "        ggtitle('Counts correlation (spearman)')+\n",
    "        theme_classic())\n",
    "    spearmanr_plot.save(f'{output_plot_prefix}_counts_valid_regions.png', height = 3, width = 16)\n",
    "    spearmanr_plot.save(f'{output_plot_prefix}_counts_valid_regions.pdf', height = 3, width = 16)\n",
    "    print(spearmanr_plot)\n",
    "    \n",
    "    plotnine.options.figure_size = (15,4)\n",
    "\n",
    "    auprc_plot = (ggplot(data = evals_df[(evals_df['metric']=='auprc')], \n",
    "                                 mapping = aes(x = 'binsize', y = 'value', color=parameter))+\n",
    "        geom_line()+\n",
    "        facet_grid('dataset ~ task')+\n",
    "        scale_y_continuous(name = 'auPRC')+\n",
    "        ggtitle('auPRC of observed versus predicted')+\n",
    "        theme_classic())\n",
    "    auprc_plot.save(f'{output_plot_prefix}_profile_valid_regions.png', height = 3, width = 16)\n",
    "    auprc_plot.save(f'{output_plot_prefix}_profile_valid_regions.pdf', height = 3, width = 16)\n",
    "    print(auprc_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_independent_assessment(glob_str = 'seq_width1000-lr0.001-lambda100-n_dil_layers7-conv_kernel_size7-tconv_kernel_size7-filters*/evaluation.valid.json',\n",
    "                           parameter = 'filters', model_dir = f'models/', output_plot_prefix = f'figures/1_model_training_and_grid_search/grid_search_filters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_independent_assessment(glob_str = 'seq_width1000-lr*-lambda100-n_dil_layers7-conv_kernel_size7-tconv_kernel_size7-filters64/evaluation.valid.json',\n",
    "                           parameter = 'lr', model_dir = f'models/', output_plot_prefix = f'figures/1_model_training_and_grid_search/grid_search_lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda\n",
    "\n",
    "Keep in mind that lambda might be different depending on the distribution reads in each sample. However, for each model we can manually compute that discrepancy to get a good idea of the correction ratio that is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_independent_assessment(glob_str = 'seq_width1000-lr0.001-lambda*-n_dil_layers7-conv_kernel_size7-tconv_kernel_size7-filters64/evaluation.valid.json',\n",
    "                           parameter = 'lambda', model_dir = f'models/', output_plot_prefix = f'figures/1_model_training_and_grid_search/grid_search_lambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute average ratio of \"counts loss to profile loss\" that each lambda would recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect evaluation metrics\n",
    "evals_path = glob(os.path.join(f'models/', 'seq_width1000-lr0.004-lambda*-n_dil_layers7-conv_kernel_size7-tconv_kernel_size7-filters64/history.csv'))\n",
    "evals_dict = {i.split('/')[1]: pd.read_csv(i) for i in evals_path}\n",
    "\n",
    "#Convert to pd.df\n",
    "evals_df = pd.DataFrame()\n",
    "for k,v in evals_dict.items():\n",
    "    v['model'] = k\n",
    "    evals_df = evals_df.append(v)\n",
    "    \n",
    "#Integrate model information\n",
    "model_params = ['seq_width','lr','lambda','n_dil_layers','conv_kernal_size','tconv_kernel_size','filters']\n",
    "evals_df[model_params] = evals_df.model.str.split('-', expand=True)\n",
    "evals_df[model_params] = evals_df[model_params].replace('[-+A-Za-z]','', regex = True).replace('_','', regex = True).replace('=','', regex = True)\n",
    "evals_df[model_params] = evals_df[model_params].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "#Return columns belonging to profile and counts\n",
    "profile_columns = [f'{t}/profile_loss' for t in tasks] + ['lambda']\n",
    "counts_columns = [f'{t}/counts_loss' for t in tasks] + ['lambda']\n",
    "\n",
    "avg_profile_loss = evals_df[profile_columns].groupby(['lambda']).mean().mean(axis=1)\n",
    "avg_counts_loss = evals_df[counts_columns].groupby(['lambda']).mean().mean(axis = 1)\n",
    "\n",
    "print(\"The average ratio of profile loss : counts_loss* lambda is:\")\n",
    "avg_profile_loss/(avg_counts_loss*avg_counts_loss.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, this means that when you want a good balance of profile:counts loss, measure lambda such that you're getting anywhere from (10:100):1. It seems like values within these ratios are optimal for improving counts predictions without sacrificing loss. Keep in mind that the loss is related to the number of reads in the samples given, so the lambda value might need to be different to accommodate the loss values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of dilational layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_independent_assessment(glob_str = 'seq_width1000-lr0.001-lambda100-n_dil_layers*-conv_kernel_size7-tconv_kernel_size7-filters64/evaluation.valid.json',\n",
    "                           parameter = 'n_dil_layers', model_dir = f'models/', output_plot_prefix = f'figures/1_model_training_and_grid_search/grid_search_ndil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train final model with best specs\n",
    "\n",
    "The model that performed best in was: `seq_width1000-lr0.001-lambda100-n_dil_layers9-conv_kernel_size7-tconv_kernel_size7-filters64`. Given these hyperparameters, generate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!python /n/projects/mw2098/shared_code/bpnet/bpnet_train_as_grid_search.py --dataspec dataspec/{prefix}_dataspec.yml \\\n",
    "--config config/default.gin --output-directory models/{prefix}/ -x .7 --manually-run \\\n",
    "--filters '64' --conv-kernel-size '7' --tconv-kernel-size '7' --n-dil-layers '9' --loss-weight '100' \\\n",
    "--learning-rate '0.001' --seq-width '1000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of best model with observed/replicate quality metrics \n",
    "\n",
    "Load the model `models/CM_vs_MN_merged_bias_exp_peaks/seq_width1000-lr0.001-lambda100-n_dil_layers9-conv_kernel_size7-tconv_kernel_size7-filters64` into this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "create_tf_session('0', .5)\n",
    "\n",
    "model = BPNetSeqModel.from_mdir(f'models/seq_width1000-lr0.001-lambda100-n_dil_layers9-conv_kernel_size7-tconv_kernel_size7-filters64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect coverage from predictions, observed, and replicate values\n",
    "\n",
    "Here, we want to compare replicate and metapeak quality metrics with the models trained below. Keep in mind that we will remove regions not associated with each task in order to ensure that we aren't predicting regions that are not of interest for particular tasks.\n",
    "\n",
    "First, collect data and regions for coverage extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_chromosomes = config['bpnet_data.exclude_chr']\n",
    "valid_chromosomes = config['bpnet_data.valid_chr']\n",
    "test_chromosomes = config['bpnet_data.test_chr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = StrandedProfile(dataspec, \n",
    "                     excl_chromosomes = excl_chromosomes + valid_chromosomes + test_chromosomes, \n",
    "                     peak_width=config['bpnet_data.peak_width'],\n",
    "                     seq_width=config['bpnet_data.seq_width'],\n",
    "                     shuffle=False)\n",
    "valid_dl = StrandedProfile(dataspec, \n",
    "                     incl_chromosomes = valid_chromosomes, \n",
    "                     excl_chromosomes = excl_chromosomes, \n",
    "                     peak_width=config['bpnet_data.peak_width'],\n",
    "                     seq_width=config['bpnet_data.seq_width'],\n",
    "                     shuffle=False)\n",
    "test_dl = StrandedProfile(dataspec, \n",
    "                     incl_chromosomes = test_chromosomes, \n",
    "                     excl_chromosomes = excl_chromosomes, \n",
    "                     peak_width=config['bpnet_data.peak_width'],\n",
    "                     seq_width=config['bpnet_data.seq_width'],\n",
    "                     shuffle=False)\n",
    "dl_dict = {'train' : train_dl,\n",
    "           'valid' : valid_dl,\n",
    "           'test' : test_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from the dataspec\n",
    "data_dict = {k: v.load_all(num_workers = 8) for k,v in dl_dict.items()}\n",
    "\n",
    "#Load region information from teh dataspec\n",
    "\n",
    "regions_dict = {k: BedTool.from_dataframe(pd.DataFrame([list(v['metadata']['range']['chr']),\n",
    "                              list(v['metadata']['range']['start']),\n",
    "                              list(v['metadata']['range']['end']),\n",
    "                              list(v['metadata']['range']['strand'])]).transpose()) for k,v in data_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect observed, replicate and metpeak coverage from the loaded data. Formats should be `{task -> [region x seqwidth x 2]}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get observed values\n",
    "y_obs_dict = {k: v['targets'] for k,v in data_dict.items()}\n",
    "\n",
    "#Get predicted values\n",
    "y_pred_seqmodel_dict = {k: model.seqmodel.predict(v['inputs']['seq']) for k,v in data_dict.items()}\n",
    "y_pred_dict = {k: {task: v[f'{task}/profile'] * np.exp(v[f'{task}/counts'][:, np.newaxis]) \n",
    "                for task in model.tasks} for  k,v in y_pred_seqmodel_dict.items()} #Provides counts*profile\n",
    "\n",
    "# Extract replicate values for each region for comparison\n",
    "rep1_obs_dict = {k: {t: np.abs(StrandedBigWigExtractor(bigwig_file = rep_path_dict[t]['rep1']).extract(v))\n",
    "            for t in model.tasks} for k,v in regions_dict.items()}\n",
    "rep2_obs_dict = {k: {t: np.abs(StrandedBigWigExtractor(bigwig_file = rep_path_dict[t]['rep2']).extract(v))\n",
    "            for t in model.tasks} for k,v in regions_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the absolute value of the replicates introduced because ChIP-nexus bigwigs have the strand information encoded as axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate counts correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the observed counts already have the `log` applied to them based off of the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpnet.metrics import pearsonr,spearmanr\n",
    "\n",
    "corr_df = pd.DataFrame()\n",
    "counts_df = pd.DataFrame()\n",
    "\n",
    "for dataset in ['train','valid','test']:\n",
    "    for task in model.tasks:\n",
    "        #Filtering regions such that they belong to the correct task at hand\n",
    "        ytrue_logcounts_filtered = np.mean(y_obs_dict[dataset][f'{task}/counts']+1, axis = 1)\n",
    "        ypred_logcounts_filtered = np.mean(y_pred_seqmodel_dict[dataset][f'{task}/counts']+1, axis = 1)\n",
    "        c_df = pd.DataFrame([ytrue_logcounts_filtered, ypred_logcounts_filtered])\n",
    "        c_df = c_df.transpose()\n",
    "        c_df.columns = ['obs','pred']\n",
    "        c_df['task'] = task\n",
    "        c_df['dataset'] = dataset\n",
    "        counts_df = counts_df.append(c_df)\n",
    "        pearson = pearsonr(ytrue_logcounts_filtered, ypred_logcounts_filtered)\n",
    "        spearman = spearmanr(ytrue_logcounts_filtered, ypred_logcounts_filtered)\n",
    "        df = pd.DataFrame([pearson, spearman, task]).transpose()\n",
    "        df.columns = ['pearson_corr','spearman_corr','task']\n",
    "        df['dataset'] = dataset\n",
    "        corr_df = corr_df.append(df)\n",
    "    corr_df['txt'] = ['corr_s: ' + str(np.round(row.spearman_corr, 2)) for i,row in corr_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plotnine.options.figure_size = (16,9)\n",
    "\n",
    "corr_plot = (ggplot(data = counts_df, mapping = aes('obs','pred'))+\n",
    "    geom_point(aes(color = 'task'), size = .4)+\n",
    "    geom_text(data = corr_df, mapping = aes(x = 9.5, y = 2.5, label = 'txt'))+\n",
    "    scale_x_continuous(name = 'log(obs_counts)')+\n",
    "    scale_y_continuous(name = 'log(pred_counts)')+\n",
    "    scale_color_manual(values = sns.color_palette('magma', n_colors = len(model.tasks)).as_hex())+\n",
    "    facet_grid('dataset ~ task')+\n",
    "    ggtitle(f'Spearman correlation across validation set')+\n",
    "    theme_classic())\n",
    "\n",
    "corr_plot.save(f'figures/1_model_training_and_grid_search/figureS3C-corr_all_regions.png', height = 6, width = 8)\n",
    "corr_plot.save(f'figures/1_model_training_and_grid_search/figureS3C-corr_all_regions.pdf', height = 6, width = 8)\n",
    "print(corr_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate replicate and metapeak auPRC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calulcate auPRC values for this model\n",
    "auprc_df = pd.DataFrame()\n",
    "for dataset in ['train','valid','test']:\n",
    "    for task in tqdm(model.tasks):\n",
    "        binsizes = [10]\n",
    "        pos_min_threshold=0.0025\n",
    "        neg_max_threshold=0.001\n",
    "        required_min_pos_counts=.01\n",
    "\n",
    "        #Filtering regions such that they belong to the correct task at hand\n",
    "        task_idx = np.where(data_dict[dataset]['metadata']['interval_from_task'] == f'{task}')[0]\n",
    "        #task_idx = range(y_obs[f'{task}/profile'].shape[0])\n",
    "\n",
    "        ##Do this for task-specific windows\n",
    "        #Observed versus predicted signal\n",
    "        o_vs_p_df = eval_seq_profile(yt = y_obs_dict[dataset][f'{task}/profile'][task_idx], \n",
    "                                     yp = y_pred_dict[dataset][f'{task}'][task_idx],\n",
    "                                 pos_min_threshold=pos_min_threshold, neg_max_threshold=neg_max_threshold,\n",
    "                                 required_min_pos_counts=required_min_pos_counts, binsizes=binsizes)\n",
    "        o_vs_p_df['type']='obs_vs_pred'\n",
    "        o_vs_p_df['task_specific']='task_specific'\n",
    "\n",
    "        #Replicate 1 vs replicate 2\n",
    "        r1_vs_r2_df = eval_seq_profile(yt = np.expand_dims(rep1_obs_dict[dataset][f'{task}'][task_idx], axis = 2), \n",
    "                                       yp = np.expand_dims(rep2_obs_dict[dataset][f'{task}'][task_idx], axis = 2),\n",
    "                                 pos_min_threshold=pos_min_threshold, neg_max_threshold=neg_max_threshold,\n",
    "                                 required_min_pos_counts=required_min_pos_counts, binsizes=binsizes)\n",
    "        r1_vs_r2_df['type']='rep1_vs_rep2'\n",
    "        r1_vs_r2_df['task_specific']='task_specific'\n",
    "\n",
    "        ## Do this for all windows\n",
    "        #Observed versus predicted signal\n",
    "        all_o_vs_p_df = eval_seq_profile(yt = y_obs_dict[dataset][f'{task}/profile'], \n",
    "                                         yp = y_pred_dict[dataset][f'{task}'],\n",
    "                                 pos_min_threshold=pos_min_threshold, neg_max_threshold=neg_max_threshold,\n",
    "                                 required_min_pos_counts=required_min_pos_counts, binsizes=binsizes)\n",
    "        all_o_vs_p_df['type']='obs_vs_pred'\n",
    "        all_o_vs_p_df['task_specific']='all_task_windows'\n",
    "\n",
    "        #Replicate 1 vs replicate 2\n",
    "        all_r1_vs_r2_df = eval_seq_profile(yt = np.expand_dims(rep1_obs_dict[dataset][f'{task}'], axis = 2), \n",
    "                                       yp = np.expand_dims(rep2_obs_dict[dataset][f'{task}'], axis = 2),\n",
    "                                 pos_min_threshold=pos_min_threshold, neg_max_threshold=neg_max_threshold,\n",
    "                                 required_min_pos_counts=required_min_pos_counts, binsizes=binsizes)\n",
    "        all_r1_vs_r2_df['type']='rep1_vs_rep2'\n",
    "        all_r1_vs_r2_df['task_specific']='all_task_windows'  \n",
    "\n",
    "        #Observed versus randomly shuffled signal\n",
    "        o_vs_r_df = o_vs_p_df.copy()\n",
    "        o_vs_r_df['auprc'] = o_vs_r_df['random_auprc']\n",
    "        o_vs_r_df['type']='obs_vs_random'\n",
    "\n",
    "        #Observed versus randomly shuffled signal\n",
    "        all_o_vs_r_df = all_o_vs_p_df.copy()\n",
    "        all_o_vs_r_df['auprc'] = all_o_vs_r_df['random_auprc']\n",
    "        all_o_vs_r_df['type']='obs_vs_random'\n",
    "\n",
    "        df = pd.concat([o_vs_p_df, r1_vs_r2_df, o_vs_r_df,\n",
    "                       all_o_vs_p_df, all_r1_vs_r2_df, all_o_vs_r_df])\n",
    "        df['task'] = task\n",
    "        df['dataset'] = dataset\n",
    "        auprc_df = auprc_df.append(df)\n",
    "auprc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the auPRC values of actual data for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotnine.options.figure_size = (8,8)\n",
    "\n",
    "auprc_df['type'] = pd.Categorical(auprc_df['type'], \n",
    "                                  categories=['obs_vs_random', 'obs_vs_pred', 'rep1_vs_rep2', 'obs_vs_metapeak'], \n",
    "                                  ordered=False)\n",
    "\n",
    "auprc_plot = (ggplot(data = auprc_df, mapping = aes(x = 'task',y='auprc'))+\n",
    "    geom_bar(mapping = aes(fill = 'type'), stat = 'identity', position = 'dodge', color = 'black')+\n",
    "    scale_fill_manual(values = ['#000000', '#b2182b', '#2166ac', '#fc8b01'])+\n",
    "    facet_grid('dataset ~ task_specific')+\n",
    "    ggtitle(f'auPRC across task-specific regions')+\n",
    "    theme_classic())\n",
    "auprc_plot.save(f'figures/1_model_training_and_grid_search/figureS3C-auprc_all_regions.png', height = 8, width = 8)\n",
    "auprc_plot.save(f'figures/1_model_training_and_grid_search/figureS3C-auprc_all_regions.pdf', height = 8, width = 8)\n",
    "print(auprc_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpnet-gpu",
   "language": "python",
   "name": "bpnet-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
